<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>What is the proper level of I/O abstraction in world models (that generate simulated rollouts)?</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            font-size: 14px;
            line-height: 1.2;
            margin: 10px;
            padding: 0;
            background-color: #f9f9f9;
            color: #333;
            max-width: 88%;
        }
        h1 {
            color: #444;
        }
        .date {
            color: #666;
            font-size: x-small
        }
        img {
            max-width: 95%;
            height: auto;
        }
    </style>
</head>
<body>
    <article>
        <p><a href="../index.html"><< back to home</a></p>
        
        <h1>What is the proper level of I/O abstraction in world models (that generate simulated rollouts)?</h1>
        <p class="date">Sonny George<br>2025-10-01</p>

        <p>As excitement builds around the idea of 'planning with simulated rollouts,' an open question remains regarding the appropriate method and degree of abstraction in world model I/O spaces.</p>

        <p>Video is an intuitive I/O space for generating simulated rollouts and contains high levels of detail/information (a picture is worth a thousand words). If we can generate realistic video, then simulating rollouts with video will indeed be viable. There is a lot of justified hype here. (See e.g., <a href="https://universal-simulator.github.io/unisim/">UniSim</a> or <a href="https://sites.google.com/view/genie-2024/">Genie</a>)</p>

        <p>However, could much of the pixel-by-pixel details in video be unnecessary for basic action—creating, at best, potentially expensive prediction overhead, and at worst, SGD distraction/dimensionality that prevents the learning of anything useful for long-tail and OOD scenarios? For example, with uncommon prompt scenarios, video models are often great at generating small details (e.g., textures) while struggling with higher-level dynamics (e.g., maintaining the physical coherence of a dynamic object).</p>

        <p>Can we map internet-scale video data into a less detailed and more general (abstract) representation space with little superfluous detail for basic action? Recently, Yann Lecun seems to be of <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">the opinion</a> that doing so is not only feasible, but also a necessity for "autonomous machine intelligence". (See e.g., <a href="https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/">V-JEPA</a>, or the planning latent space of <a href="https://danijar.com/project/daydreamer/">DayDreamer</a>.)</p>
            
        <p>Roboticists attempt this when they, before learning control policies, distill image/lidar data into assortments of pertinent 3d representations (maps, way points, point clouds, voxels, etc.). Could agent-action trajectories in internet-scale video be reliably converted into sufficiently descriptive 3d representations?</p>

        <p>Whether converting the training video to (1) (an assortment of) 3d representations or (2) some other representation space with a theoretically higher (relevant-) signal-to-noise ratio, what stands to be gained/lost?</p>
            
        <p>Besides (perhaps) ameliorating the signal-to-noise ratio, could converting massive video datasets to 3d representations have other benefits for training world models? E.g.,</p>
            <p style="margin:5px;margin-left:20px">(1) Cross-embodiment positive learning transfer—not being confined to a single agent-embodiment (by conditioning generation on some agent-embodiment specification<span style="color: red;">*</span>)
            <br>(2) Easier embodiment-specific inverse dynamics models from outputs (by again, specifying the agent-embodiment in the input<span style="color: red;">*</span> and getting outputs that, in theory, should map directly to the specified embodiment)
            <br>(3) More stable training via loss penalties for physical abnormalities (e.g. volumetric or joint-range impossibilities)
            <br>(4) Enabling safety measures—e.g., preventing generations with potential collisions that approach a force threshold
            </p>
        
        <p style="margin: 20px;"></p>
        <img src="../assets/imgs/diagram_of_abstraction_levels_for_world_models.png" alt="Diagram of abstraction levels for world models">
    </article>
</body>
</html>
